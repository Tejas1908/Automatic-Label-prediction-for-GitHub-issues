{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "# from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "import logging\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVG='micro'\n",
    "y_true=np.array([1,0,1,1])\n",
    "y_pred=np.array([0,1,1,0])\n",
    "print(precision_score(y_pred,y_true))\n",
    "print(recall_score(y_true,y_pred))\n",
    "print(f1_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a random seed in order to keep the same random results each time I run the notebook\n",
    "np.random.seed(seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEJAS\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Updated At</th>\n",
       "      <th>Closed At</th>\n",
       "      <th>HTML URL</th>\n",
       "      <th>Status</th>\n",
       "      <th>is_locked</th>\n",
       "      <th>Role</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603077341.0</td>\n",
       "      <td>b'[SPARK-31489][SPARK-31488][SQL] Translate da...</td>\n",
       "      <td>b'### What changes were proposed in this pull ...</td>\n",
       "      <td>2020-04-20T09:16:13Z</td>\n",
       "      <td>2020-04-20T09:24:37Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/apache/spark/pull/28272</td>\n",
       "      <td>open</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>b'SQL'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>603063137.0</td>\n",
       "      <td>b'[SPARK-31495][SQL] Support formatted explain...</td>\n",
       "      <td>b\"&lt;!--\\r\\nThanks for sending a pull request!  ...</td>\n",
       "      <td>2020-04-20T08:56:30Z</td>\n",
       "      <td>2020-04-20T09:03:07Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/apache/spark/pull/28271</td>\n",
       "      <td>open</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>b'SQL'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>602931061.0</td>\n",
       "      <td>b'[SPARK-31494][ML] flatten the result datafra...</td>\n",
       "      <td>b'### What changes were proposed in this pull ...</td>\n",
       "      <td>2020-04-20T04:32:21Z</td>\n",
       "      <td>2020-04-20T06:09:06Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/apache/spark/pull/28270</td>\n",
       "      <td>open</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>b'ML'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>602921805.0</td>\n",
       "      <td>b'[SPARK-31493][SQL] Optimize InSet to In acco...</td>\n",
       "      <td>b\"&lt;!--\\r\\nThanks for sending a pull request!  ...</td>\n",
       "      <td>2020-04-20T04:02:56Z</td>\n",
       "      <td>2020-04-20T07:39:06Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/apache/spark/pull/28269</td>\n",
       "      <td>open</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>b'SQL'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>602917700.0</td>\n",
       "      <td>b'[SPARK-31492][ML] flatten the result datafra...</td>\n",
       "      <td>b'### What changes were proposed in this pull ...</td>\n",
       "      <td>2020-04-20T03:48:38Z</td>\n",
       "      <td>2020-04-20T05:31:07Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/apache/spark/pull/28268</td>\n",
       "      <td>open</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>b'ML'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id                                              Title  \\\n",
       "0  603077341.0  b'[SPARK-31489][SPARK-31488][SQL] Translate da...   \n",
       "1  603063137.0  b'[SPARK-31495][SQL] Support formatted explain...   \n",
       "2  602931061.0  b'[SPARK-31494][ML] flatten the result datafra...   \n",
       "3  602921805.0  b'[SPARK-31493][SQL] Optimize InSet to In acco...   \n",
       "4  602917700.0  b'[SPARK-31492][ML] flatten the result datafra...   \n",
       "\n",
       "                                                Body            Created At  \\\n",
       "0  b'### What changes were proposed in this pull ...  2020-04-20T09:16:13Z   \n",
       "1  b\"<!--\\r\\nThanks for sending a pull request!  ...  2020-04-20T08:56:30Z   \n",
       "2  b'### What changes were proposed in this pull ...  2020-04-20T04:32:21Z   \n",
       "3  b\"<!--\\r\\nThanks for sending a pull request!  ...  2020-04-20T04:02:56Z   \n",
       "4  b'### What changes were proposed in this pull ...  2020-04-20T03:48:38Z   \n",
       "\n",
       "             Updated At Closed At                                    HTML URL  \\\n",
       "0  2020-04-20T09:24:37Z       NaN  https://github.com/apache/spark/pull/28272   \n",
       "1  2020-04-20T09:03:07Z       NaN  https://github.com/apache/spark/pull/28271   \n",
       "2  2020-04-20T06:09:06Z       NaN  https://github.com/apache/spark/pull/28270   \n",
       "3  2020-04-20T07:39:06Z       NaN  https://github.com/apache/spark/pull/28269   \n",
       "4  2020-04-20T05:31:07Z       NaN  https://github.com/apache/spark/pull/28268   \n",
       "\n",
       "  Status is_locked         Role  Labels  \n",
       "0   open     FALSE  CONTRIBUTOR  b'SQL'  \n",
       "1   open     FALSE  CONTRIBUTOR  b'SQL'  \n",
       "2   open     FALSE  CONTRIBUTOR   b'ML'  \n",
       "3   open     FALSE  CONTRIBUTOR  b'SQL'  \n",
       "4   open     FALSE  CONTRIBUTOR   b'ML'  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the data into a dataframe\n",
    "issues=pd.read_csv('../Curated issues-100.csv')\n",
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 164732 entries, 0 to 164731\n",
      "Data columns (total 11 columns):\n",
      "Id            164731 non-null float64\n",
      "Title         164732 non-null object\n",
      "Body          164732 non-null object\n",
      "Created At    164730 non-null object\n",
      "Updated At    164731 non-null object\n",
      "Closed At     1 non-null object\n",
      "HTML URL      164731 non-null object\n",
      "Status        164731 non-null object\n",
      "is_locked     164731 non-null object\n",
      "Role          164730 non-null object\n",
      "Labels        164730 non-null object\n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 13.8+ MB\n"
     ]
    }
   ],
   "source": [
    "issues.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking=issues[issues['Body'].isna()]\n",
    "# checking.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues.drop(columns=['Created At', 'Updated At', 'Closed At',\n",
    "#                      'HTML URL',\n",
    "                     'Status','is_locked','Role'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues.dropna(inplace=True)\n",
    "issues.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Labels'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utf82str(text):\n",
    "    text=text.strip('b').strip(\"'\")\n",
    "    return text\n",
    "\n",
    "def split_labels(labels):\n",
    "    return labels.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utf82str(issues['Title'].iloc[0]))\n",
    "print((issues['Title'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Title']=issues['Title'].apply(lambda x: utf82str(x))\n",
    "issues['Body']=issues['Body'].apply(lambda x: utf82str(x))\n",
    "issues['Labels']=issues['Labels'].apply(lambda x: utf82str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "issues.isnull().mean(axis=0).plot.barh()\n",
    "plt.title(\"Ratio of missing values per columns\")\n",
    "# new_df.isnull().mean(axis=0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Duplicate entries: {}'.format(issues.duplicated().sum()))\n",
    "issues.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Processing the Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. convert to lowercase\n",
    "2. remove left numbers\n",
    "3. remove <>\n",
    "4. replace : with / (if doesn't work try replacing / with :)\n",
    "5. if \\\\xf0\\\\x9f\\\\x99\\\\x82 are there remove the label itself\n",
    "6. remove [Type]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Labels']=issues['Labels'].apply(lambda x: x.lower())\n",
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Labels']=issues['Labels'].apply(lambda x: split_labels(x))\n",
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_numbers(label_list):\n",
    "    new_label_list=[]\n",
    "    for label in label_list:\n",
    "        new_label_list.append(label.lstrip('0123456789.-_:/ '))\n",
    "        \n",
    "    return new_label_list\n",
    "\n",
    "\n",
    "def strip_numbers(label_list):\n",
    "    new_label_list=[]\n",
    "    for label in label_list:\n",
    "        new_label_list.append(label.strip('0123456789.-_ '))\n",
    "        \n",
    "    return new_label_list\n",
    "\n",
    "def remove_unwanted(label_list):\n",
    "    new_label_list=[]\n",
    "    for label in label_list:\n",
    "#         temp=re.sub(r'\\[?type\\]?','',label)\n",
    "        temp=re.sub(r\"[<>]\", \"\", label)\n",
    "        new_label_list.append(temp)\n",
    "        \n",
    "    return new_label_list\n",
    "\n",
    "def replace_char_with_char(label_list):\n",
    "    new_label_list=[]\n",
    "    for label in label_list:\n",
    "        new_label_list.append(re.sub(r\":\", \"/\", label))\n",
    "        \n",
    "    return new_label_list\n",
    "\n",
    "def remove_unencoded(label_list):\n",
    "    new_label_list=[]\n",
    "    for label in label_list:\n",
    "        temp=re.sub(r\"\\\\x\",'',label)\n",
    "        if temp==label:\n",
    "            new_label_list.append(label)\n",
    "        \n",
    "    return new_label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Labels']=issues['Labels'].apply(lambda x: remove_unwanted(x))\n",
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Labels']=issues['Labels'].apply(lambda x: remove_leading_numbers(x))\n",
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Labels']=issues['Labels'].apply(lambda x: remove_unencoded(x))\n",
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Labels']=issues['Labels'].apply(lambda x: replace_char_with_char(x))\n",
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Labels'].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [item for sublist in issues['Labels'].values for item in sublist]\n",
    "print(len(all_labels))\n",
    "# print(all_labels[13546:14600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing null labels from the label list\n",
    "\n",
    "temp=pd.DataFrame(all_labels,columns=['labels'])\n",
    "temp=temp[temp['labels']!='']\n",
    "all_labels=list(temp['labels'])\n",
    "print(len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels=list(set(all_labels))\n",
    "print(len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_list = [item for sublist in new_df['Tags'].values for item in sublist]\n",
    "\n",
    "keywords = nltk.FreqDist(all_labels)\n",
    "\n",
    "# print(keywords.items())\n",
    "keywords = nltk.FreqDist(keywords)\n",
    "# print(keywords.most_common(5))\n",
    "frequencies_words = keywords.most_common(100)\n",
    "labels_features = [word[0] for word in frequencies_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=keywords.most_common(100)\n",
    "for word in check:\n",
    "    print(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels_features))\n",
    "feature_check=' '.join(labels_features)\n",
    "# print(feature_check)\n",
    "# print('ing ci_' in feature_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "keywords.plot(10, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(labels):\n",
    "    labels_filtered = []\n",
    "    for i in range(0, len(labels)):\n",
    "        if labels[i] in labels_features:\n",
    "            labels_filtered.append(labels[i])\n",
    "    return labels_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Labels']=issues['Labels'].apply(lambda x: most_common(x))\n",
    "issues['Labels']=issues['Labels'].apply(lambda x: x if len(x)>0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(issues.shape)\n",
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Labels'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues.dropna(subset=['Labels'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Body'].iloc[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing Body preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of code snippets,error logs and shit\n",
    "def remove_screenshots(text):\n",
    "    return re.sub(r\"```(.*?)```\", '__________', temp, flags=re.MULTILINE)\n",
    "\n",
    "# getting rid of people tags\n",
    "def remove_people_tags(text):\n",
    "    return re.sub(r\"@[a-zA-Z0-9_.-]+\",' ',text)\n",
    "\n",
    "# getting rid of urls\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"https?\\S+\", ' ', text, flags=re.MULTILINE)\n",
    "\n",
    "# getting rid of \\n and \\r\n",
    "def remove_escape_seqs(text):\n",
    "    return re.sub(r\"\\\\r|\\\\n\", ' ', text, flags=re.MULTILINE)\n",
    "\n",
    "# removing numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r\"[0-9]+\", ' ', text, flags=re.MULTILINE)\n",
    "\n",
    "# remove single letters\n",
    "def remove_single_letters(sentence):\n",
    "    return ' '.join( [w for w in sentence.split() if len(w)>1] )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=ToktokTokenizer()\n",
    "punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_list_noempty(mylist):\n",
    "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
    "    return [item for item in newlist if item != '']\n",
    "\n",
    "def clean_punct(text): \n",
    "    words=token.tokenize(text)\n",
    "    punctuation_filtered = []\n",
    "    regex = re.compile('[%s]' % re.escape(punct))\n",
    "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
    "    for w in words:\n",
    "        if w in labels_features:\n",
    "            punctuation_filtered.append(w)\n",
    "        else:\n",
    "            punctuation_filtered.append(regex.sub(' ', w))\n",
    "  \n",
    "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
    "        \n",
    "    return ' '.join(map(str, filtered_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Body'].iloc[12345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=0\n",
    "temp=issues['Body'].iloc[sample]\n",
    "print(temp)\n",
    "print(issues['HTML URL'].iloc[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Actual Body and title processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Title'] = issues['Title'].apply(lambda x: BeautifulSoup(x).get_text()) \n",
    "issues['Title'] = issues['Title'].apply(lambda x: remove_people_tags(x)) \n",
    "issues['Title'] = issues['Title'].apply(lambda x: remove_url(x))\n",
    "issues['Title'] = issues['Title'].apply(lambda x: remove_escape_seqs(x)) \n",
    "issues['Title'] = issues['Title'].apply(lambda x: clean_text(x)) \n",
    "issues['Title'] = issues['Title'].apply(lambda x: clean_punct(x)) \n",
    "issues['Title'] = issues['Title'].apply(lambda x: remove_numbers(x)) \n",
    "issues['Title'] = issues['Title'].apply(lambda x: remove_single_letters(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Body'] = issues['Body'].apply(lambda x: BeautifulSoup(x).get_text()) \n",
    "# issues['Body'] = issues['Body'].apply(lambda x: remove_screenshots(x))\n",
    "issues['Body'] = issues['Body'].apply(lambda x: remove_people_tags(x))\n",
    "issues['Body'] = issues['Body'].apply(lambda x: remove_url(x))\n",
    "issues['Body'] = issues['Body'].apply(lambda x: remove_escape_seqs(x))\n",
    "issues['Body'] = issues['Body'].apply(lambda x: clean_text(x))\n",
    "issues['Body'] = issues['Body'].apply(lambda x: clean_punct(x))\n",
    "issues['Body'] = issues['Body'].apply(lambda x: remove_numbers(x))\n",
    "issues['Body'] = issues['Body'].apply(lambda x: remove_single_letters(x)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Body'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add more stop words<br>\n",
    "dont remove labels from title or body**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# stop_words= list(stop_words)+['what','pr','is','this','does','it','we',\n",
    "#                               'eg','cc','module','happens','happened',\n",
    "#                               'issue','why','environment','os','etc']\n",
    "\n",
    "def lemitizeWords(text):\n",
    "    words=token.tokenize(text)\n",
    "    listLemma=[]\n",
    "    for w in words:\n",
    "        x=lemma.lemmatize(w, pos=\"v\")\n",
    "        listLemma.append(x)\n",
    "    return ' '.join(map(str, listLemma))\n",
    "\n",
    "\n",
    "def stopWordsRemove(text):\n",
    "#     stop_words = set(stopwords.words(\"english\"))\n",
    "    words=token.tokenize(text)\n",
    "    filtered = [w for w in words if w in feature_check or not w in stop_words]\n",
    "    \n",
    "    return ' '.join(map(str, filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Title'] = issues['Title'].apply(lambda x: lemitizeWords(x)) \n",
    "issues['Title'] = issues['Title'].apply(lambda x: stopWordsRemove(x)) \n",
    "issues['Body'] = issues['Body'].apply(lambda x: lemitizeWords(x)) \n",
    "issues['Body'] = issues['Body'].apply(lambda x: stopWordsRemove(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['Body'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 20\n",
    "text = issues['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_train = TfidfVectorizer(analyzer = 'word',\n",
    "                                       min_df=0.0,\n",
    "                                       max_df = 1.0,\n",
    "                                       strip_accents = None,\n",
    "                                       encoding = 'utf-8', \n",
    "                                       preprocessor=None,\n",
    "                                       token_pattern=r\"(?u)\\S\\S+\", # Need to repeat token pattern\n",
    "                                       max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_matrix = vectorizer_train.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer_train.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TF_IDF_matrix.shape)\n",
    "temp=TF_IDF_matrix\n",
    "dense=temp.todense()\n",
    "# print(dense)\n",
    "denselist=dense.tolist()\n",
    "# print(denselist)\n",
    "\n",
    "tfidf=pd.DataFrame(denselist,columns=vectorizer_train.get_feature_names())\n",
    "print(len(tfidf))\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer_train.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50,random_state=11).fit(TF_IDF_matrix)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"--------------------------------------------\")\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        print(\"--------------------------------------------\")\n",
    "        \n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, vectorizer_train.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = issues['Body']\n",
    "X2 = issues['Title']\n",
    "y = issues['Labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "y_bin = multilabel_binarizer.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_X1 = TfidfVectorizer(analyzer = 'word',\n",
    "                                       min_df=0.0,\n",
    "                                       max_df = 1.0,\n",
    "                                       strip_accents = None,\n",
    "                                       encoding = 'utf-8', \n",
    "                                       preprocessor=None,\n",
    "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
    "                                       max_features=1000)\n",
    "\n",
    "vectorizer_X2 = TfidfVectorizer(analyzer = 'word',\n",
    "                                       min_df=0.0,\n",
    "                                       max_df = 1.0,\n",
    "                                       strip_accents = None,\n",
    "                                       encoding = 'utf-8', \n",
    "                                       preprocessor=None,\n",
    "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
    "                                       max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_tfidf = vectorizer_X1.fit_transform(X1)\n",
    "X2_tfidf = vectorizer_X2.fit_transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vectorizer_X1.vocabulary_))\n",
    "print(len(vectorizer_X2.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X1_tfidf.shape)\n",
    "print(X2_tfidf.shape)\n",
    "X_tfidf = hstack([X1_tfidf,X2_tfidf])\n",
    "# X_tfidf=X2_tfidf\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = 0.2, random_state = 0) # Do 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_jacard(y_true,y_pred):\n",
    "    '''\n",
    "    see https://en.wikipedia.org/wiki/Multi-label_classification#Statistics_and_evaluation_metrics\n",
    "    '''\n",
    "    jacard = np.minimum(y_true,y_pred).sum(axis=1) / np.maximum(y_true,y_pred).sum(axis=1)\n",
    "    \n",
    "    return jacard.mean()*100\n",
    "\n",
    "def print_score(y_pred, clf):\n",
    "    print(\"Clf: \", clf.__class__.__name__)\n",
    "    print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n",
    "    print(\"Precision: {}\".format(precision_score(y_test, y_pred,average='samples')*100))\n",
    "    print(\"Recall: {}\".format(recall_score(y_test, y_pred,average='samples')*100))\n",
    "    print(\"F score: {}\".format(f1_score(y_test, y_pred,average='samples')*100))\n",
    "    print(\"Hamming loss: {}\".format(hamming_loss(y_test, y_pred)*100))\n",
    "    \n",
    "    print(\"---\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 46 54\n",
    "dummy = DummyClassifier()\n",
    "sgd = SGDClassifier()\n",
    "lr = LogisticRegression()\n",
    "mn = MultinomialNB()\n",
    "svc = LinearSVC()  # 10chains 0.15thresh f1 60.3\n",
    "                    # 15chains 0.15thresh f1 60.6\n",
    "perceptron = Perceptron()  # 10chains 0.25thresh f1 56\n",
    "pac = PassiveAggressiveClassifier()  # 10chains 0.25thresh f1 58\n",
    "\n",
    "for classifier in [dummy, sgd, lr, mn, svc, perceptron, pac]:\n",
    "    clf = OneVsRestClassifier(classifier)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print_score(y_pred, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an independent logistic regression model for each class using the\n",
    "# OneVsRestClassifier wrapper.\n",
    "base_lr = LinearSVC()\n",
    "ovr = OneVsRestClassifier(base_lr)\n",
    "ovr.fit(X_train, y_train)\n",
    "Y_pred_ovr = ovr.predict(X_test)\n",
    "ovr_f1_score = f1_score(y_test, Y_pred_ovr, average='samples')\n",
    "ovr_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an ensemble of logistic regression classifier chains and take the\n",
    "# take the average prediction of all the chains.\n",
    "chains = [ClassifierChain(base_lr, order='random', random_state=i) for i in range(10)]\n",
    "i=1\n",
    "for chain in chains:\n",
    "    print(i)\n",
    "    i+=1\n",
    "    chain.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_chains = np.array([chain.predict(X_test) for chain in chains])\n",
    "Y_pred_ensemble = Y_pred_chains.mean(axis=0)\n",
    "thresholds=np.arange(0.01,0.9,.01)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    ensemble_f1_score = f1_score(y_test,\n",
    "                                       Y_pred_ensemble >= thresh,\n",
    "                                       average='samples')\n",
    "    print(np.round(thresh,2),ensemble_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.15\n",
    "# y_pred =model.predict(X_test)\n",
    "# print(y_pred[0])\n",
    "# print(y_test[0])\n",
    "y_pred_new=np.zeros(Y_pred_ensemble.shape)\n",
    "\n",
    "for i in range(Y_pred_ensemble.shape[0]):\n",
    "    for j in range(Y_pred_ensemble.shape[1]):\n",
    "        \n",
    "        if Y_pred_ensemble[i,j]>=threshold:\n",
    "            y_pred_new[i,j]=1\n",
    "                   \n",
    "ensemble_f1_score = f1_score(y_test,\n",
    "                                       Y_pred_ensemble >= threshold,\n",
    "                                       average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_f1_scores = [f1_score(y_test, Y_pred_chain >= threshold,\n",
    "                                      average='samples')\n",
    "                        for Y_pred_chain in Y_pred_chains]\n",
    "\n",
    "\n",
    "model_scores = [ovr_f1_score] + chain_f1_scores\n",
    "model_scores.append(ensemble_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_names = ('Independent',\n",
    "               'Chain 1',\n",
    "               'Chain 2',\n",
    "               'Chain 3',\n",
    "               'Chain 4',\n",
    "               'Chain 5',\n",
    "               'Chain 6',\n",
    "               'Chain 7',\n",
    "               'Chain 8',\n",
    "               'Chain 9',\n",
    "               'Chain 10',\n",
    "               'Ensemble')\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "\n",
    "# Plot the Jaccard similarity scores for the independent model, each of the\n",
    "# chains, and the ensemble (note that the vertical axis on this plot does\n",
    "# not begin at 0).\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.grid(True)\n",
    "ax.set_title('Classifier Chain Ensemble Performance Comparison')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(model_names, rotation='vertical')\n",
    "ax.set_ylabel('F1 Similarity Score')\n",
    "ax.set_ylim([min(model_scores) * .9, max(model_scores) * 1.1])\n",
    "colors = ['r'] + ['b'] * len(chain_f1_scores) + ['g']\n",
    "ax.bar(x_pos, model_scores, alpha=0.5, color=colors)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(y_pred_new,ovr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_new.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample=123\n",
    "\n",
    "groundtruth=[]\n",
    "for i in range(len(y_test[test_sample])):\n",
    "    if y_test[test_sample][i]==1:\n",
    "        groundtruth.append(multilabel_binarizer.classes_[i])\n",
    "        \n",
    "predicted=y_pred_new[test_sample]\n",
    "# print(len(predicted))\n",
    "predicted_labels=[]\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i]==1:\n",
    "        predicted_labels.append(multilabel_binarizer.classes_[i])\n",
    "\n",
    "print(groundtruth)\n",
    "print(predicted_labels)\n",
    "\n",
    "print('Common labels : ',list(set(groundtruth) & set(predicted_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "# 200 100 2000 feats thresh 0.5| j47 p55 f53\n",
    "# 200 100 2000 feats thresh 0.05| j49 p54 f56\n",
    "# 400 100 2000 feats thresh 0.05| j50 p54 f57\n",
    "# 200 100 2000 feats thresh 0.1| j50 p55 f56\n",
    "# 100     2000 feats thresh 0.25| j49 p54 f56\n",
    "# 400 200 100 2000 feats thresh 0.1 |j49 p55 f55\n",
    "model = Sequential()\n",
    "model.add(Dense(4000, input_dim=2000, activation='relu'))\n",
    "# model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(100,\n",
    "#                 input_dim=2000,\n",
    "                activation='sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=model.evaluate(X_test,y_test)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =model.predict(X_test)\n",
    "thresholds=np.arange(0.01,0.9,.01)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    fscore = f1_score(y_test,y_pred >= thresh,\n",
    "                                       average='samples')\n",
    "    print(np.round(thresh,2),fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.08\n",
    "# print(y_pred[0])\n",
    "# print(y_test[0])\n",
    "y_pred_new=np.zeros(y_pred.shape)\n",
    "\n",
    "for i in range(y_pred.shape[0]):\n",
    "    for j in range(y_pred.shape[1]):\n",
    "        \n",
    "        if y_pred[i,j]>=threshold:\n",
    "            y_pred_new[i,j]=1\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(y_pred_new,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 400,100 : j49 p57 f54\n",
    "# # 200,100 : j50 p60\n",
    "# # 100     : j48 p56 f53\n",
    "# # 100,100 : j48 p56 f53\n",
    "# mlpc = MLPClassifier(hidden_layer_sizes=(200,100),\n",
    "#                      activation='relu',\n",
    "#                      verbose=True,\n",
    "#                      early_stopping=True,\n",
    "#                      validation_fraction=0.1,\n",
    "#                      n_iter_no_change=10,\n",
    "#                      learning_rate='adaptive',\n",
    "#                      max_iter=25)\n",
    "# mlpc.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = mlpc.predict(X_test)\n",
    "\n",
    "# print_score(y_pred, mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print_score(y_pred, rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample=189\n",
    "\n",
    "groundtruth=[]\n",
    "for i in range(len(y_test[test_sample])):\n",
    "    if y_test[test_sample][i]==1:\n",
    "        groundtruth.append(multilabel_binarizer.classes_[i])\n",
    "        \n",
    "predicted=ovr.predict(X_test[test_sample])[0]\n",
    "# print(len(predicted))\n",
    "predicted_labels=[]\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i]==1:\n",
    "        predicted_labels.append(multilabel_binarizer.classes_[i])\n",
    "\n",
    "print(groundtruth)\n",
    "print(predicted_labels)\n",
    "\n",
    "print('Common labels : ',list(set(groundtruth) & set(predicted_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_train.shape[1]):\n",
    "    print(multilabel_binarizer.classes_[i])\n",
    "    print(confusion_matrix(y_test[:,i], y_pred[:,i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top10(feature_names, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"--------------------------------------------\")\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" \".join(feature_names[j] for j in top10)))\n",
    "        print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer_X1.get_feature_names() + vectorizer_X2.get_feature_names()\n",
    "# feature_names = vectorizer_X2.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top10(feature_names, ovr, multilabel_binarizer.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
